<div align="center">

<!-- Hero SVG Banner -->
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 900 200" width="900" height="200">
  <defs>
    <linearGradient id="bg" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#0f0c29"/>
      <stop offset="50%" style="stop-color:#302b63"/>
      <stop offset="100%" style="stop-color:#24243e"/>
    </linearGradient>
    <linearGradient id="accent" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#f953c6"/>
      <stop offset="100%" style="stop-color:#b91d73"/>
    </linearGradient>
    <filter id="glow">
      <feGaussianBlur stdDeviation="3" result="coloredBlur"/>
      <feMerge>
        <feMergeNode in="coloredBlur"/>
        <feMergeNode in="SourceGraphic"/>
      </feMerge>
    </filter>
  </defs>
  <!-- Background -->
  <rect width="900" height="200" fill="url(#bg)" rx="12"/>
  <!-- Decorative film strip left -->
  <rect x="20" y="20" width="30" height="160" fill="#1a1a2e" rx="4"/>
  <rect x="23" y="30" width="24" height="16" fill="#f953c6" rx="2" opacity="0.8"/>
  <rect x="23" y="56" width="24" height="16" fill="#b91d73" rx="2" opacity="0.8"/>
  <rect x="23" y="82" width="24" height="16" fill="#f953c6" rx="2" opacity="0.8"/>
  <rect x="23" y="108" width="24" height="16" fill="#b91d73" rx="2" opacity="0.8"/>
  <rect x="23" y="134" width="24" height="16" fill="#f953c6" rx="2" opacity="0.8"/>
  <rect x="23" y="160" width="24" height="16" fill="#b91d73" rx="2" opacity="0.6"/>
  <!-- Decorative film strip right -->
  <rect x="850" y="20" width="30" height="160" fill="#1a1a2e" rx="4"/>
  <rect x="853" y="30" width="24" height="16" fill="#f953c6" rx="2" opacity="0.8"/>
  <rect x="853" y="56" width="24" height="16" fill="#b91d73" rx="2" opacity="0.8"/>
  <rect x="853" y="82" width="24" height="16" fill="#f953c6" rx="2" opacity="0.8"/>
  <rect x="853" y="108" width="24" height="16" fill="#b91d73" rx="2" opacity="0.8"/>
  <rect x="853" y="134" width="24" height="16" fill="#f953c6" rx="2" opacity="0.8"/>
  <rect x="853" y="160" width="24" height="16" fill="#b91d73" rx="2" opacity="0.6"/>
  <!-- Shield icon -->
  <path d="M430 40 L450 50 L450 80 Q450 95 440 100 Q430 105 420 100 Q410 95 410 80 L410 50 Z"
        fill="none" stroke="url(#accent)" stroke-width="2.5" filter="url(#glow)"/>
  <text x="430" y="80" text-anchor="middle" fill="#f953c6" font-size="22" font-weight="bold"
        filter="url(#glow)">‚úì</text>
  <!-- Main title -->
  <text x="460" y="85" text-anchor="start" fill="white" font-size="40" font-weight="900"
        font-family="'Segoe UI', Arial, sans-serif" filter="url(#glow)">video-nsfw</text>
  <!-- Subtitle -->
  <text x="450" y="130" text-anchor="middle" fill="#c9b8f5" font-size="16"
        font-family="'Segoe UI', Arial, sans-serif" letter-spacing="2">
    AI-Powered Video Content Moderation
  </text>
  <!-- Accent line -->
  <rect x="200" y="148" width="500" height="2" fill="url(#accent)" rx="1" opacity="0.7"/>
  <!-- Tech tags -->
  <rect x="255" y="158" width="90" height="22" fill="#f953c6" rx="11" opacity="0.15"/>
  <text x="300" y="174" text-anchor="middle" fill="#f953c6" font-size="11"
        font-family="monospace">PyTorch</text>
  <rect x="360" y="158" width="90" height="22" fill="#f953c6" rx="11" opacity="0.15"/>
  <text x="405" y="174" text-anchor="middle" fill="#f953c6" font-size="11"
        font-family="monospace">Transformers</text>
  <rect x="465" y="158" width="90" height="22" fill="#f953c6" rx="11" opacity="0.15"/>
  <text x="510" y="174" text-anchor="middle" fill="#f953c6" font-size="11"
        font-family="monospace">OpenCV</text>
  <rect x="570" y="158" width="90" height="22" fill="#f953c6" rx="11" opacity="0.15"/>
  <text x="615" y="174" text-anchor="middle" fill="#f953c6" font-size="11"
        font-family="monospace">ViT Model</text>
</svg>

# video-nsfw

[![Python](https://img.shields.io/badge/Python-3.9%2B-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![HuggingFace](https://img.shields.io/badge/ü§ó%20Transformers-4.35%2B-FFD21F?style=for-the-badge)](https://huggingface.co/transformers/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.8%2B-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)](https://opencv.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![Version](https://img.shields.io/badge/Version-1.0.0-blue?style=for-the-badge)](https://github.com/Kaelith69/video-nsfw)
[![CUDA](https://img.shields.io/badge/CUDA-Compatible-76B900?style=for-the-badge&logo=nvidia&logoColor=white)](https://developer.nvidia.com/cuda-toolkit)

> **AI-powered video content moderation** ‚Äî automatically detects NSFW material in videos by sampling and classifying frames using a Vision Transformer (ViT) model fine-tuned on NSFW detection.

</div>

---

## üìñ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Project Structure](#-project-structure)
- [Requirements](#-requirements)
- [Installation](#-installation)
- [Usage](#-usage)
  - [Command-Line](#command-line)
  - [Python API](#python-api)
- [Configuration](#-configuration)
- [Output Schema](#-output-schema)
- [How It Works](#-how-it-works)
- [Performance Notes](#-performance-notes)
- [Contributing](#-contributing)
- [License](#-license)

---

## ‚ú® Features

| Feature | Description |
|---|---|
| üé¨ **Frame Sampling** | Intelligently samples N evenly-spaced frames from any video |
| ü§ñ **ViT Classification** | Uses `Falconsai/nsfw_image_detection` Vision Transformer |
| ‚ö° **GPU Acceleration** | Automatically uses CUDA when available, falls back to CPU |
| üõë **Early Stop** | Optionally halts analysis on first NSFW detection |
| üéØ **Configurable Threshold** | Tune confidence threshold to balance precision/recall |
| üìä **Rich Results** | Per-frame predictions, confidence scores, and summary verdict |
| üîí **Safe Resource Handling** | Video capture is always released via `try/finally` |

---

## üèõ Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   VideoContentAnalyzer                   ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   __init__   ‚îÇ    ‚îÇ        analyze_video()        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Load Model ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 1. Open video (OpenCV)       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Load Proc. ‚îÇ    ‚îÇ 2. Compute frame interval    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Set device ‚îÇ    ‚îÇ 3. Loop: seek ‚Üí read ‚Üí PIL   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ 4. Call analyze_frame()      ‚îÇ   ‚îÇ
‚îÇ                      ‚îÇ 5. Aggregate results         ‚îÇ   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ 6. Early-stop if NSFW+flag   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇanalyze_frame ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Preprocess ‚îÇ                                        ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Inference  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Softmax    ‚îÇ    ‚îÇ    HuggingFace Transformers   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ id2label   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Falconsai/nsfw_image_detect  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                         ‚îÇ
   OpenCV (video I/O)         PyTorch (inference)
```

**Data flow:**

```
Video File
    ‚îÇ
    ‚ñº
cv2.VideoCapture ‚îÄ‚îÄ‚ñ∫ Frame (BGR ndarray)
    ‚îÇ
    ‚ñº
cv2.cvtColor (BGR‚ÜíRGB) ‚îÄ‚îÄ‚ñ∫ PIL.Image
    ‚îÇ
    ‚ñº
ViTImageProcessor ‚îÄ‚îÄ‚ñ∫ Tensor batch (on device)
    ‚îÇ
    ‚ñº
AutoModelForImageClassification ‚îÄ‚îÄ‚ñ∫ logits
    ‚îÇ
    ‚ñº
torch.softmax ‚îÄ‚îÄ‚ñ∫ probabilities
    ‚îÇ
    ‚ñº
argmax + id2label ‚îÄ‚îÄ‚ñ∫ ("nsfw"/"normal", confidence)
    ‚îÇ
    ‚ñº
Results dict
```

---

## üìÅ Project Structure

```
video-nsfw/
‚îú‚îÄ‚îÄ video_analyzer.py      # Main module ‚Äî VideoContentAnalyzer class + CLI entry point
‚îú‚îÄ‚îÄ requirements.txt       # Pinned Python dependencies
‚îî‚îÄ‚îÄ README.md              # This file
```

---

## üì¶ Requirements

- Python **3.9+**
- `opencv-python` >= 4.8.0
- `torch` >= 2.0.0
- `Pillow` >= 10.0.0
- `transformers` >= 4.35.0
- *(Optional)* NVIDIA GPU + CUDA for accelerated inference

---

## üöÄ Installation

```bash
# 1. Clone the repository
git clone https://github.com/Kaelith69/video-nsfw.git
cd video-nsfw

# 2. (Recommended) Create a virtual environment
python -m venv .venv
source .venv/bin/activate      # Linux / macOS
# .venv\Scripts\activate       # Windows

# 3. Install dependencies
pip install -r requirements.txt
```

> **GPU users:** Replace the `torch` install with the CUDA-enabled wheel from [pytorch.org](https://pytorch.org/get-started/locally/).
>
> ```bash
> pip install torch --index-url https://download.pytorch.org/whl/cu121
> ```

---

## üéÆ Usage

### Command-Line

```bash
# Analyze a local video file (results printed to stdout)
python video_analyzer.py /path/to/your/video.mp4
```

**Example output:**

```
NSFW content detected!
First occurrence: Frame 2
Confidence: 94.31%

Detailed frame analysis:
Frame 1: normal (99.12% confidence)
Frame 2: nsfw (94.31% confidence)
```

### Python API

```python
from pathlib import Path
from video_analyzer import VideoContentAnalyzer

# Initialize (downloads model on first run ~85 MB)
analyzer = VideoContentAnalyzer()

# Analyze with defaults (6 frames, stop on NSFW, 50% threshold)
results = analyzer.analyze_video("my_video.mp4")

# Analyze more frames without early stopping
results = analyzer.analyze_video(
    video_path=Path("my_video.mp4"),
    num_frames=12,
    stop_on_nsfw=False,
    nsfw_threshold=0.7,
)

# Check overall verdict
if results["nsfw_detected"]:
    frame = results["first_nsfw_frame"]
    print(f"NSFW detected at frame {frame['frame_number']} "
          f"({frame['confidence']:.2%} confidence)")

# Iterate per-frame results
for frame in results["frame_results"]:
    print(f"Frame {frame['frame_number']}: {frame['prediction']} "
          f"@ {frame['confidence']:.2%}")
```

---

## ‚öôÔ∏è Configuration

| Parameter | Type | Default | Description |
|---|---|---|---|
| `model_name` | `str` | `"Falconsai/nsfw_image_detection"` | HuggingFace model ID or local path |
| `num_frames` | `int` | `6` | Number of frames to sample from the video |
| `stop_on_nsfw` | `bool` | `True` | Stop immediately on first NSFW detection |
| `nsfw_threshold` | `float` | `0.5` | Minimum confidence to flag as NSFW (0.0‚Äì1.0) |

---

## üìä Output Schema

`analyze_video()` returns a `dict` with the following structure:

```python
{
    "frames_analyzed": int,          # Number of frames successfully analyzed
    "nsfw_detected": bool,           # True if any frame exceeded the threshold
    "first_nsfw_frame": {            # None if no NSFW detected
        "frame_number": int,         #   1-based index in the sampled set
        "position": int,             #   Absolute frame position in the video
        "prediction": str,           #   "nsfw" or "normal"
        "confidence": float,         #   Softmax probability (0.0‚Äì1.0)
    },
    "frame_results": [               # One entry per analyzed frame
        {
            "frame_number": int,
            "position": int,
            "prediction": str,
            "confidence": float,
        },
        # ...
    ],
}
```

---

## üî¨ How It Works

1. **Frame Sampling** ‚Äî The video's total frame count is divided evenly by `num_frames` to compute a sampling `interval`. Frames at positions `0, interval, 2√óinterval, ‚Ä¶` are selected.

2. **BGR ‚Üí RGB** ‚Äî OpenCV reads frames in BGR color order. The frame is converted to RGB before being passed to the ViT processor, which was trained on RGB images.

3. **ViT Inference** ‚Äî Each PIL image is tokenised by `ViTImageProcessor` into a patch-based tensor. The model returns raw `logits` over its label vocabulary (`nsfw`, `normal`).

4. **Softmax + Label** ‚Äî `torch.softmax` converts logits to probabilities. The `argmax` of the logits selects the winning class, and `id2label` maps the integer index back to a human-readable string.

5. **Early Stop** ‚Äî If `stop_on_nsfw=True` and the predicted label is `"nsfw"` with `confidence >= nsfw_threshold`, analysis halts immediately to save time and compute.

---

> üí° *Place a quirky GIF here ‚Äî something like a robot scanning film frames ‚Äî to break up the wall of text and bring some ‚ú® personality ‚ú® to the docs.*

---

## ‚ö° Performance Notes

- **First run** downloads the model weights (~85 MB) from HuggingFace Hub and caches them locally.
- **GPU inference** is dramatically faster. On an NVIDIA RTX 3080, each frame classifies in ~5 ms vs ~80 ms on CPU.
- Increasing `num_frames` improves accuracy at the cost of linear time increase.
- The `stop_on_nsfw=True` default keeps worst-case latency low for clearly NSFW content.

---

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/my-improvement`)
3. Commit your changes (`git commit -m 'Add some improvement'`)
4. Push to the branch (`git push origin feature/my-improvement`)
5. Open a Pull Request

---

## üìÑ License

This project is licensed under the **MIT License**. See [LICENSE](LICENSE) for details.

---

<div align="center">

*Made with ‚ù§Ô∏è and a healthy fear of unmoderated content*

---

**Why did the neural network go to therapy?**
*It had too many deep issues it couldn't resolve on its own.* ü•Å

</div>
